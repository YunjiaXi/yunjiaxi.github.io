---
title: "Efficient and Deployable Knowledge Infusion for Open-World Recommendations via Large Language Models"
collection: publications
category: manuscripts
permalink: /publication/2025-reki
excerpt: 'Yunjia Xi, Weiwen Liu, Jianghao Lin, Muyan Weng, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming TANG, Yong Yu, Weinan Zhang'
date: 2025-05-25
venue: 'ACM Transactions on Recommender Systems
 (TORS)'
paperurl: 'https://www.arxiv.org/pdf/2408.10520'
---

Recommender system plays a pervasive role in today’s online services, yet its closed-loop nature, i.e., training and deploying within a specific closed domain, constrains its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating advanced reasoning capabilities. However, previous attempts to directly implement LLMs as recommenders fall short in meeting the demanding requirements of industrial recommender systems, particularly in terms of online inference latency and offline resource efficiency. In this work, we propose an Open-World Recommendation Framework with Efficient and Deployable Knowledge Infusion from Large Language Models, dubbed REKI, to acquire two types of external knowledge about users and items from LLMs. Specifically, we introduce factorization prompting to elicit accurate knowledge reasoning on user preferences and items. With factorization prompting, we develop individual knowledge extraction and collective knowledge extraction tailored for different scales of recommendation scenarios, effectively reducing offline resource consumption. Subsequently, the generated user and item knowledge undergoes efficient transformation and condensation into augmented vectors through a hybridized expert-integrated network, ensuring its compatibility with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any conventional recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments demonstrate that REKI significantly outperforms the state-of-the-art baselines and is compatible with a diverse array of recommendation algorithms and tasks. Now, REKI has been deployed to Huawei’s news and music recommendation platforms and gained a 7% and 1.99% improvement during the online A/B test.